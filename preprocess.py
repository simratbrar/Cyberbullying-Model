# -*- coding: utf-8 -*-
"""
Created on Mon Aug 13 16:54:38 2020

@author: Brar
"""
import pandas as pd
import numpy as np
import re
from nltk import word_tokenize

#constants
YES='Yes'
NO='No'
LETTERS='abcdefghijklmnopqrstuvwxyz'



def clean_post(post):
    #change text to lowercase
    post=post.lower()
    
    #remove question tag
    post=re.sub('q:','',post)
    
    #remove answer tag
    post=re.sub('a:','',post)
    
    #remove <br> tag
    post=re.sub('<br>','',post)
    
    #remove &quot tag
    post=re.sub('&quot','',post)
    
    #filter only alphanumeric characters
    post=re.sub(r'[^a-zA-Z ]+','',post)
    
    return post
    
def filter_token(token):
    #remove multiple continuous recurring letters
    for letter in LETTERS:
        pattern=letter+'{2,}'
        token=re.sub(pattern,letter+letter,token)
    return token

def get_dataset(path='Dataset/formspring_data.csv'):
    data = pd.read_csv(path, error_bad_lines=False,sep='\t',
                   names=['userid','post','ques','ans','asker','ans1','severity1','bully1','ans2',
                          'severity2','bully2','ans3','severity3','bully3'])
    
    for ind in range(1,len(data)):
        if(data.loc[ind,'ans1']==YES):
            dataset.append((clean_post(str(data.loc[ind,'bully1'])),YES))
        elif (data.loc[ind,'ans2']==YES):
            dataset.append((clean_post(str(data.loc[ind,'bully2'])),YES))
        elif (data.loc[ind,'ans3']==YES):
            dataset.append((clean_post(str(data.loc[ind,'bully3'])),YES))
        else:
            dataset.append((clean_post(str(data.loc[ind,'post'])),NO))
    return dataset


def tokenize_dataset(dataset):
    data_tokenized=[]
    for post,label in dataset:
        post=word_tokenize(post)
        post=[filter_token(token) for token in post]
        data_tokenized.append((post,label))
    return data_tokenized

#store all tokens in a set sort set to a list
def get_vocabulary_list(data_tokenized):
    vocab=set()
    for post,_ in data_tokenized:
        for word in post:
            vocab.add(word)
    
    vocabulary=list(vocab)
    vocabulary=sorted(vocabulary)
    return vocabulary

#write list to file for espeak
def write_list_to_file(vocabulary):
    import os
    os.chdir('C:\Program Files (x86)\eSpeak\command_line')
    counter=0
    for word in vocabulary:
        with open(str(counter)+'.txt','w') as file:
            file.write(word)
        file.close()
        counter+=1

#read phoneme vocabulary file generated by espeak for each word
def get_word_to_phoneme_dict(vocabulary):
    word_to_phoneme_dict={}

    for i in range(18065):
        with open(str(i)+'out.txt','r') as file:
            phoneme=file.read()
            phoneme=phoneme.strip()
            word_to_phoneme_dict[vocabulary[i]]=phoneme
        file.close()

def write_word_to_phoneme_dict(word_to_phoneme_dict,path='wordtophoneme.csv'):
    with open(path,'w') as file:
        for word,phoneme in word_to_phoneme_dict.items():
            file.write(word+'\t'+phoneme+'\n')
        file.close()

def read_word_to_phoneme(path='Dataset/wordtophoneme.csv'):
    word_phoneme_df=pd.read_csv(path,sep='\t',names=['word','phoneme'])
    word_to_phoneme_dict={}
    for index in range(len(word_phoneme_df)):
        word=word_phoneme_df.loc[index,'word']
        phoneme=word_phoneme_df.loc[index,'phoneme']
        word_to_phoneme_dict[word]=phoneme
    return word_to_phoneme_dict

def get_tokenized_dataset():
    dataset=get_dataset()
    tokenized_dataset=tokenize_dataset(dataset)
    return tokenized_dataset




